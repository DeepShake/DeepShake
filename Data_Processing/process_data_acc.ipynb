{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "This notebook goes through the steps to process raw .pkl files into a usable format by\n",
    "1. Extracting data for each earthquake as well as earthquake metadata\n",
    "2. Selecting a subset of stations out of all those with data to perform analysis\n",
    "3. Compress data via subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "matplotlib.rc('font', **{'size': 18})\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '../data/Ridgecrest/'\n",
    "data_dir = \"../data/\"\n",
    "raw_path = os.path.join(data_dir, \"raw\")\n",
    "meta_path = os.path.join(data_dir, \"metadata\")\n",
    "compressed_path = os.path.join(data_dir, \"compressed\")\n",
    "relevant_path = os.path.join(data_dir, \"relevant_stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1: Extract data for earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a: Extract accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(station_start_time, earthquake_start_time, period = 6000, step=100, offset=15):\n",
    "    start_idx = int(100*(earthquake_start_time - station_start_time)) - offset*step\n",
    "    end_idx = start_idx + period\n",
    "    return (start_idx, end_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(load_path)\n",
    "\n",
    "output_dir = Path(raw_path)\n",
    "output_files = [os.path.split(f)[1].split('.')[0] for f in sorted(output_dir.glob(\"*.npy\"))]\n",
    "print(output_files)\n",
    "\n",
    "if not os.path.exists(raw_path):\n",
    "    os.mkdir(raw_path)\n",
    "\n",
    "def np_from_pkl(data_path, save_path, period = 6000):\n",
    "    input_dir = Path(data_path)\n",
    "    sorted(input_dir.glob(\"*.pkl\"))\n",
    "    for f in tqdm(sorted(input_dir.glob(\"*.pkl\"))):\n",
    "#         print(f)\n",
    "        day_id = os.path.split(f)[1].split('.')[0]\n",
    "#         if day_id in output_files: \n",
    "#             print(\"File found for date: {}\".format(day_id))\n",
    "#             continue\n",
    "        ## Iterating over pickles\n",
    "        stations = []\n",
    "        with open(f, 'rb') as fp:\n",
    "            try:\n",
    "                data = pickle.load(fp)\n",
    "            except:\n",
    "                print(\"Pickle Corrupted\")\n",
    "                print(\"Corrupted path: {}\".format(f))\n",
    "                continue\n",
    "            num_stations, num_events = len(data['stations']), len(data['events'])\n",
    "#             data_arr = np.zeros\n",
    "\n",
    "            for station in data['stations']:\n",
    "                events = []\n",
    "                for event in data['events']:\n",
    "                    start_idx, end_idx = get_indices(data['stations'][station]['starttime'], event['time'])\n",
    "                    if (end_idx > len(data['stations'][station]['data']) - 1) or (start_idx < 0):\n",
    "                        continue #skip event\n",
    "                    events.append(np.transpose(data['stations'][station]['data'][start_idx: end_idx], (1, 0)))\n",
    "                events = np.array(events)\n",
    "#                 print(\"events shape\", events.shape)\n",
    "                stations.append(events)\n",
    "        save_arr = np.array(stations) \n",
    "#         print(\"station shape: \", save_arr.shape)\n",
    "        filename = day_id + \".npy\"\n",
    "        np.save(os.path.join(save_path, filename), save_arr) \n",
    "        \n",
    "np_from_pkl(load_path, raw_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b: Extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reads through pickles. Saves stations and events (w/o the big boy quake data)    \n",
    "\n",
    "if not os.path.exists(meta_path):\n",
    "    os.mkdir(meta_path)\n",
    "    \n",
    "def save_station_and_events(data_path, save_path):\n",
    "    dropped = []\n",
    "    input_dir = Path(data_path)\n",
    "    sorted(input_dir.glob(\"*.pkl\"))\n",
    "    for f in tqdm(sorted(input_dir.glob(\"*.pkl\"))):\n",
    "        print(f)\n",
    "        day_id = os.path.split(f)[1].split('.')[0]\n",
    "        ## Iterating over pickles\n",
    "#         stations = []\n",
    "        with open(f, 'rb') as fp:\n",
    "            try:\n",
    "                data = pickle.load(fp)\n",
    "            except:\n",
    "                print(\"Pickle Corrupted\")\n",
    "                print(\"Corrupted path: {}\".format(f))\n",
    "                continue\n",
    "                \n",
    "        first_stat = list(data['stations'].keys())[0]\n",
    "        filename = day_id + \"_meta.json\"\n",
    "                \n",
    "        ## Idxs to keep\n",
    "        idxs_to_keep = []\n",
    "        for e, event in enumerate(data['events']):\n",
    "            start_idx, end_idx = get_indices(data['stations'][first_stat]['starttime'], event['time'])\n",
    "            if (end_idx > len(data['stations'][first_stat]['data']) - 1) or (start_idx < 0):\n",
    "                ## Event was skipped\n",
    "                pass\n",
    "            else:\n",
    "                idxs_to_keep.append(e)\n",
    "        before = len(data['events'])\n",
    "        data['events'] = [data['events'][idx] for idx in idxs_to_keep]\n",
    "        after = len(data['events'])\n",
    "        print(\"Num events dropped from date {}: {}\".format(day_id, before - after))\n",
    "        dropped.append(before - after)\n",
    "        for station in data['stations']:\n",
    "#             print(data['stations'][station].keys())\n",
    "            for key in ['starttime', 'endtime', 'data']:\n",
    "                del data['stations'][station][key]            \n",
    "        for e, event in enumerate(data['events']):\n",
    "            del data['events'][e]['time']\n",
    "#         print(data)\n",
    "        stations = np.array([stat for stat in data['stations'].keys()])\n",
    "        print(stations)\n",
    "        np.save(os.path.join(save_path, \"{}_stations.npy\".format(day_id)), stations)\n",
    "        with open(os.path.join(save_path, filename), 'w') as fp:\n",
    "            json.dump(data, fp)      \n",
    "    with open(os.path.join(save_path, \"dropped_events.txt\"), 'w') as fp:\n",
    "        fp.write(str(sum(dropped)) + '\\n')\n",
    "        for d in dropped:\n",
    "            fp.write(str(d)+'\\n')\n",
    "        \n",
    "save_station_and_events(load_path, meta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c: Count number of quakes per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-09-18_stations.npy',\n",
       " '2019-09-23_stations.npy',\n",
       " '2019-08-03_stations.npy',\n",
       " '2019-08-31_stations.npy',\n",
       " '2019-07-17_stations.npy',\n",
       " '2019-06-03_stations.npy',\n",
       " '2019-06-26_stations.npy',\n",
       " '2019-07-16_stations.npy',\n",
       " '2019-09-29_stations.npy',\n",
       " '2019-06-17_stations.npy',\n",
       " '2019-06-04_stations.npy',\n",
       " '2019-09-28_stations.npy',\n",
       " '2019-07-02_stations.npy',\n",
       " '2019-09-25_stations.npy',\n",
       " '2019-06-29_stations.npy',\n",
       " '2019-06-08_stations.npy',\n",
       " '2019-09-06_stations.npy',\n",
       " '2019-09-14_stations.npy',\n",
       " '2019-06-01_stations.npy',\n",
       " '2019-07-05_stations.npy',\n",
       " '2019-09-16_stations.npy',\n",
       " '2019-07-14_stations.npy',\n",
       " '2019-06-13_stations.npy',\n",
       " '2019-08-14_stations.npy',\n",
       " '2019-09-17_stations.npy',\n",
       " '2019-09-27_stations.npy',\n",
       " '2019-08-04_stations.npy',\n",
       " '2019-09-22_stations.npy',\n",
       " '2019-07-18_stations.npy',\n",
       " '2019-07-30_stations.npy',\n",
       " '2019-08-27_stations.npy',\n",
       " '2019-09-15_stations.npy',\n",
       " '2019-09-05_stations.npy',\n",
       " '2019-06-06_stations.npy',\n",
       " '2019-09-12_stations.npy',\n",
       " '2019-09-21_stations.npy',\n",
       " '2019-06-09_stations.npy',\n",
       " '2019-07-19_stations.npy',\n",
       " '2019-08-19_stations.npy',\n",
       " '2019-08-11_stations.npy',\n",
       " '2019-06-21_stations.npy',\n",
       " '2019-09-19_stations.npy',\n",
       " '2019-07-21_stations.npy',\n",
       " '2019-08-25_stations.npy',\n",
       " '2019-08-10_stations.npy',\n",
       " '2019-07-23_stations.npy',\n",
       " '2019-09-24_stations.npy',\n",
       " '2019-09-03_stations.npy',\n",
       " '2019-07-24_stations.npy',\n",
       " '2019-06-11_stations.npy',\n",
       " '2019-08-18_stations.npy',\n",
       " '2019-06-15_stations.npy',\n",
       " '2019-08-30_stations.npy',\n",
       " '2019-08-21_stations.npy',\n",
       " '2019-08-15_stations.npy',\n",
       " '2019-07-09_stations.npy',\n",
       " '2019-08-16_stations.npy',\n",
       " '2019-07-22_stations.npy',\n",
       " '2019-07-20_stations.npy',\n",
       " '2019-08-07_stations.npy',\n",
       " '2019-06-27_stations.npy',\n",
       " '2019-07-26_stations.npy',\n",
       " '2019-06-02_stations.npy',\n",
       " '2019-08-26_stations.npy',\n",
       " '2019-07-07_stations.npy',\n",
       " '2019-09-09_stations.npy',\n",
       " '2019-08-02_stations.npy',\n",
       " '2019-07-29_stations.npy',\n",
       " '2019-06-12_stations.npy',\n",
       " '2019-09-04_stations.npy',\n",
       " '2019-08-12_stations.npy',\n",
       " '2019-08-01_stations.npy',\n",
       " '2019-06-07_stations.npy',\n",
       " '2019-06-16_stations.npy',\n",
       " '2019-07-11_stations.npy',\n",
       " '2019-07-03_stations.npy',\n",
       " '2019-06-18_stations.npy',\n",
       " '2019-06-19_stations.npy',\n",
       " '2019-08-06_stations.npy',\n",
       " '2019-07-04_stations.npy',\n",
       " '2019-09-30_stations.npy',\n",
       " '2019-07-01_stations.npy',\n",
       " '2019-08-24_stations.npy',\n",
       " '2019-08-09_stations.npy',\n",
       " '2019-08-22_stations.npy',\n",
       " '2019-06-10_stations.npy',\n",
       " '2019-09-11_stations.npy',\n",
       " '2019-08-13_stations.npy',\n",
       " '2019-09-20_stations.npy',\n",
       " '2019-07-25_stations.npy',\n",
       " '2019-08-05_stations.npy',\n",
       " '2019-09-10_stations.npy',\n",
       " '2019-08-17_stations.npy',\n",
       " '2019-09-13_stations.npy',\n",
       " '2019-09-07_stations.npy',\n",
       " '2019-09-26_stations.npy',\n",
       " '2019-06-30_stations.npy',\n",
       " '2019-06-25_stations.npy',\n",
       " '2019-07-06_stations.npy',\n",
       " '2019-07-08_stations.npy',\n",
       " '2019-09-01_stations.npy',\n",
       " '2019-06-05_stations.npy',\n",
       " '2019-07-27_stations.npy',\n",
       " '2019-07-15_stations.npy',\n",
       " '2019-08-28_stations.npy',\n",
       " '2019-08-08_stations.npy',\n",
       " '2019-06-23_stations.npy',\n",
       " '2019-07-31_stations.npy',\n",
       " '2019-07-13_stations.npy',\n",
       " '2019-06-22_stations.npy',\n",
       " '2019-07-12_stations.npy',\n",
       " '2019-06-14_stations.npy',\n",
       " '2019-09-02_stations.npy',\n",
       " '2019-07-10_stations.npy',\n",
       " '2019-06-24_stations.npy',\n",
       " '2019-08-20_stations.npy',\n",
       " '2019-06-20_stations.npy',\n",
       " '2019-08-23_stations.npy',\n",
       " '2019-06-28_stations.npy',\n",
       " '2019-09-08_stations.npy',\n",
       " '2019-08-29_stations.npy',\n",
       " '2019-07-28_stations.npy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_files = [entry.name for entry in Path(meta_path).glob(\"*.npy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = []\n",
    "for file in station_files:\n",
    "    stations.append(np.load(os.path.join(meta_path, file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for v in stations:\n",
    "    for stat in v:\n",
    "        if stat not in counts:\n",
    "            counts[stat] = 0\n",
    "print(\"Stations: \", counts)\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_idx, s in enumerate(shapes):\n",
    "    num_events = s[1]\n",
    "    assert s[0] == len(stations[dates[s_idx]])\n",
    "    for stat in stations[dates[s_idx]]:\n",
    "        counts[stat] += num_events\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted(zip(list(counts.values()), list(counts.keys())))\n",
    "sorted_counts, sorted_stats = zip(*sorted_dict)\n",
    "sorted_counts, sorted_stats = sorted_counts[::-1], sorted_stats[::-1]\n",
    "plt.figure(figsize=(40,10))\n",
    "plt.bar(sorted_stats, sorted_counts)\n",
    "plt.xlabel('Stations'); plt.ylabel(\"Num events\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-2: Filter relevant stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stations = 15\n",
    "\n",
    "with open('./all_station_counts.json') as json_file:\n",
    "    stat_counts = json.load(json_file)\n",
    "\n",
    "pprint(stat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted(zip(list(stat_counts.values()), list(stat_counts.keys())))\n",
    "sorted_counts, sorted_stats = zip(*sorted_dict)\n",
    "sorted_counts, sorted_stats = sorted_counts[::-1], sorted_stats[::-1]\n",
    "plt.bar(sorted_stats, sorted_counts)\n",
    "plt.title(\"Station event counts\")\n",
    "plt.show()\n",
    "\n",
    "master_stations = sorted_stats[:num_stations]\n",
    "print(master_stations)\n",
    "print(sorted_counts[:num_stations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create array of dates\n",
    "dates = sorted([entry.name.split(\".\")[0] for entry in list(os.scandir(raw_path)) if \"npy\" in entry.name])\n",
    "print(sorted(dates))\n",
    "print(len(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(relevant_path):\n",
    "    os.mkdir(relevant_path)\n",
    "\n",
    "print(\"Extracting data for stations: {}\".format(master_stations))\n",
    "np.save(os.path.join(relevant_path, \"stations.npy\"), master_stations)\n",
    "\n",
    "\n",
    "for date in tqdm(dates):\n",
    "    date_data = os.path.join(raw_path, date+\".npy\")\n",
    "#     print(\"Loading data from date {}\".format(date))\n",
    "    date_arr = np.load(date_data)\n",
    "    \n",
    "    ## look for stations for that day (metadata)\n",
    "    station_meta = os.path.join(meta_path, date+\"_stations.npy\")\n",
    "#     print(\"\\tLoading metadata from {}\".format(station_meta))\n",
    "    date_stations = np.load(station_meta)\n",
    "    delete_date = False\n",
    "    stats_for_date = []\n",
    "#     print(\"\\tStations for date {}: {}\".format(date, date_stations))\n",
    "\n",
    "    for master in master_stations: \n",
    "        if master not in date_stations: \n",
    "            delete_date = True\n",
    "            break            \n",
    "        idx_date = date_arr[date_stations == master]\n",
    "#         print(idx)\n",
    "#         stats_for_date.append(date_arr[idx, :, :,])\n",
    "        stats_for_date.append(idx_date)\n",
    "\n",
    "    if delete_date is True: \n",
    "        print(\"\\tDeleting date {}\".format(date))\n",
    "        continue\n",
    "    stats_for_date = np.concatenate(stats_for_date, 0)\n",
    "    assert stats_for_date.shape[0] == len(master_stations)\n",
    "    date_save = os.path.join(relevant_path, \"{}.npy\".format(date))\n",
    "    stats_for_date = np.transpose(stats_for_date, [1, 0, 2, 3]) #(num_evs, stats, dim, time)\n",
    "    stats_for_date = np.linalg.norm(stats_for_date, ord=2, axis=2)\n",
    "#     print(stats_for_date.shape)\n",
    "    np.save(date_save, stats_for_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-3: Compress via subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(compressed_path):\n",
    "    os.mkdir(compressed_path)\n",
    "    \n",
    "files = [entry.name for entry in os.scandir(relevant_path) if \"2019\" in entry.name]\n",
    "print(files)\n",
    "print(len(files))\n",
    "\n",
    "WIDTH = 100\n",
    "for file in files:\n",
    "    print(\"Processing file: {}\".format(file))\n",
    "    large = np.load(os.path.join(load_path, file))\n",
    "    print(\"\\tBefore: \", large.shape)\n",
    "    comp_shape = (large.shape[0], large.shape[1], large.shape[2] // WIDTH)\n",
    "    compressed = np.zeros(comp_shape)\n",
    "    for i in range(comp_shape[2]):\n",
    "        compressed[:, :, i] = np.mean(large[:, :, WIDTH*i: WIDTH*(i+1)], 2)\n",
    "    print(\"\\tAfter: \", compressed.shape)\n",
    "    np.save(os.path.join(compressed_path, file), compressed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date = \"2019-07-06.npy\"\n",
    "test_original = np.load(os.path.join(relevant_path, test_date))\n",
    "test_compressed = np.load(os.path.join(compressed_path, test_date))\n",
    "\n",
    "plt.plot(test[0, 0, :], alpha=0.25)\n",
    "plt.show()\n",
    "plt.plot(compressed[0, 0, :])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
