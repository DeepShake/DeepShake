{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pa49bUnKyRgF"
   },
   "source": [
    "# DeepShake Multi-step Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GU8C5qm_4vZb"
   },
   "source": [
    "This notebook is a template for doing time series forecasting on the Ridgecrest dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import py3nvml\n",
    "sys.path.append(\"./networks/.\")\n",
    "sys.path.append(\"../utils/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "import py3nvml\n",
    "\n",
    "from utils import *\n",
    "from lstm import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.backend import categorical_crossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ad9697/anaconda3/envs/tf_gpu/bin/python\n",
      "/home/ad9697/anaconda3/envs/tf_gpu/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n",
    "\n",
    "# # print(py3nvml.grab_gpus(1))\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 28 20:40:17 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    37W / 250W |   8255MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    37W / 250W |  15961MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    37W / 250W |   4774MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    39W / 250W |   1998MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     81304      C   metashape                                    313MiB |\n",
      "|    0    192473      C   /usr/bin/python3.6                          4825MiB |\n",
      "|    0    245352      C   ...obal/miniconda3/envs/pth-1.4/bin/python  3103MiB |\n",
      "|    1     52559      C   /home/danjwu/anaconda3/bin/python          15243MiB |\n",
      "|    1     81304      C   metashape                                    313MiB |\n",
      "|    2     17874      C   ...al/miniconda3/envs/tf-1.10.0/bin/python  1105MiB |\n",
      "|    2     24352      C   /usr/bin/python3.6                          1671MiB |\n",
      "|    2     81304      C   metashape                                    313MiB |\n",
      "|    2    225363      C   /usr/bin/python3.6                          1671MiB |\n",
      "|    3     81304      C   metashape                                    313MiB |\n",
      "|    3    235010      C   /usr/bin/python3.6                          1671MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad9697/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/py3nvml/utils.py:151: RuntimeWarning: Could not find enough GPUs for your job\n",
      "  warnings.warn(\"Could not find enough GPUs for your job\", RuntimeWarning)\n",
      "GPU 3:\tUsed Mem: 1998.3125MB\tTotal Mem: 16160.5MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "py3nvml.grab_gpus(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TokBlnUhWFw9"
   },
   "source": [
    "## The Ridgecrest Dataset and parsed data\n",
    "This notebook uses a earthquake dataset from Ridgecrest, collected from July to September of 2019.\n",
    "There are several thousand earthquakes during this time period, which were collected at anywhere from 16-30 stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stations = 15\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/class_mmi\"\n",
    "\n",
    "x_train_multi = np.load(os.path.join(data_path, \"X_train_unnorm.npy\"))\n",
    "x_val_multi = np.load(os.path.join(data_path, \"X_val_unnorm.npy\"))\n",
    "\n",
    "y_train = np.load(os.path.join(data_path, \"y_train.npy\"))\n",
    "y_val = np.load(os.path.join(data_path, \"y_val.npy\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1998010, 15), (499520, 15))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# # creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "y_train_OH = onehotencoder.fit_transform(y_train.reshape((-1, 1))).\\\n",
    "reshape((-1, num_classes*num_stations)).toarray()\n",
    "y_val_OH = onehotencoder.fit_transform(y_val.reshape((-1, 1))).\\\n",
    "reshape((-1, num_classes*num_stations)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\"y_val_OH.npy\"), y_val_OH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14985075 14861315   112700     9240     1680      140]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3746400 3718400   25515    1645     595     245]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Class prevalence\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(counts)\n",
    "plt.bar(unique, np.log(counts))\n",
    "plt.show()\n",
    "\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(counts)\n",
    "plt.bar(unique, np.log(counts))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaler = preprocessing.StandardScaler().fit(x_train_multi)\n",
    "# # mean_scaler = preprocessing.StandardScaler().fit(train_mean)\n",
    "# # std_scaler = preprocessing.StandardScaler().fit(train_std)\n",
    "\n",
    "# x_train_multi = X_scaler.transform(x_train_multi)\n",
    "# # mean_scaled_train = mean_scaler.transform(train_mean)\n",
    "# # std_scaled_train = std_scaler.transform(train_std)\n",
    "\n",
    "# x_val_multi = X_scaler.transform(x_val_multi)\n",
    "# # mean_scaled_val = mean_scaler.transform(val_mean)\n",
    "# # std_scaled_val = std_scaler.transform(val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/class_mmi/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-481dd0eb7647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/class_mmi/config.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config_path = os.path.join(data_path, \"config.json\")\n",
    "with open(config_path) as json_file:\n",
    "    config = json.load(json_file)\n",
    "    \n",
    "_, past_history, STEP, future_target= list(config.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GnE087bJYSu"
   },
   "source": [
    "### Model with unnormalized log(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 15, 128)           73728     \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 15, 64)            33024     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 360)               345960    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 90)                32490     \n",
      "=================================================================\n",
      "Total params: 534,610\n",
      "Trainable params: 534,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from tf.keras.backend import sparse_categorical_crossentropy\n",
    "# from tf.keras.backend import categorical_crossentropy\n",
    "# from tf.keras.metrics import categorical_accuracy\n",
    "\n",
    "def get_multipred_crossentropy_loss_fn(predictions = 15, classes = 6):\n",
    "    '''\n",
    "    Returns a loss which expects a predictions * classes vector, reshapes, and sums up the individual categorical crossentropies\n",
    "    '''\n",
    "    def loss(y_true, y_pred):\n",
    "        total_loss = 0\n",
    "        for i in range(0, classes*predictions, classes):\n",
    "            total_loss += categorical_crossentropy(y_true[i:i + classes], y_pred[i : i+classes])\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def multipred_acc(y_true, y_pred):\n",
    "    '''\n",
    "    Returns an acc which expects a predictions * classes vector, reshapes\n",
    "    '''\n",
    "    avg_acc = 0\n",
    "    for i in range(0, num_classes*num_stations, num_classes):\n",
    "        avg_acc += categorical_accuracy(y_true[i:i + num_classes], y_pred[i : i+num_classes])\n",
    "        \n",
    "    avg_acc = avg_acc / num_stations\n",
    "    return avg_acc\n",
    "\n",
    "# def get_multipred_softmax_fn(predictions = 15, classes = 6):\n",
    "#     '''\n",
    "#     Expects a predictions * classes vector, reshapes to a 2d (predictions, classes), takes row-wise softmax, and flattens\n",
    "#     '''\n",
    "def multipred_softmax(x):\n",
    "    output = K.reshape(K.softmax(K.reshape(x, (-1, num_stations, num_classes)), axis = -1), (-1, num_classes*num_stations))\n",
    "    return output\n",
    "    \n",
    "#     return multipred_softmax\n",
    "\n",
    "# def get_multipred_crossentropy_loss_fn(predictions = 15, classes = 6):\n",
    "#     '''\n",
    "#     Returns a loss which expects a predictions * classes vector, reshapes, and sums up the individual categorical crossentropies\n",
    "#     '''\n",
    "#     def loss(y_true, y_pred):\n",
    "#         print(y_true.shape, y_pred.shape)\n",
    "#         total_loss = 0\n",
    "#         for i in range(0, classes*predictions, classes):\n",
    "#             total_loss += sparse_categorical_crossentropy(y_true[int(i // classes)], y_pred[i : i+classes])\n",
    "#         return total_loss\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "def model_v1():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(128,\n",
    "                                return_sequences=True,\n",
    "                                input_shape=x_train_multi.shape[-2:]))\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(tf.keras.layers.LSTM(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(num_stations*num_classes, activation=Activation(multipred_softmax, name='custom_softmax')))\n",
    "\n",
    "    # model.load_weights(model_path)\n",
    "\n",
    "    model.compile(optimizer='nadam', loss=get_multipred_crossentropy_loss_fn(), metrics=[multipred_acc])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def model_v2():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(128,\n",
    "                                return_sequences=True,\n",
    "                                input_shape=x_train_multi.shape[-2:]))\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(tf.keras.layers.LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(num_stations*num_classes*4, activation='relu'))   \n",
    "    model.add(tf.keras.layers.Dense(num_stations*num_classes, activation=Activation(multipred_softmax, name='custom_softmax')))\n",
    "\n",
    "    # model.load_weights(model_path)\n",
    "\n",
    "    model.compile(optimizer='nadam', loss=get_multipred_crossentropy_loss_fn(), metrics=[multipred_acc])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = model_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.backend import variable\n",
    "\n",
    "# y_true = np.zeros((15, 6))\n",
    "# y_pred = y_true.copy()\n",
    "# y_true[:7, 1] = 1\n",
    "# y_true[7:, 2] = 1\n",
    "# y_pred[:, 1] = 1\n",
    "# print(y_pred, y_true)\n",
    "# y_pred = variable(y_pred.flatten())\n",
    "# y_true = variable(y_true.flatten())\n",
    "\n",
    "# multipred_acc(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,CSVLogger\n",
    "from keras import optimizers\n",
    "\n",
    "lr = .01\n",
    "BATCH_SIZE=2048\n",
    "save_path = \"./trained_models_acc/mmi_classification\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# cur_file = \"LSTM3_10steps_dropzero\"\n",
    "cur_file = 'classifier_v2'\n",
    "cur_file = os.path.join(save_path, cur_file)\n",
    "\n",
    "opt = optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_multipred_acc',\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_multipred_acc',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "model_check = ModelCheckpoint(cur_file + '.h5', monitor='val_multipred_acc', verbose=1, save_best_only=True, \n",
    "                              save_weights_only=True, mode='auto')\n",
    "csv_log = CSVLogger(cur_file+ '_history.csv')\n",
    "callback_list = [early_stop, reduce_lr, model_check, csv_log]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvB7zBqVSMyl"
   },
   "source": [
    "Let's see how the model predicts before it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preds_tensor = variable(test_preds)\n",
    "# y_val_OH_tensor = variable(y_val_OH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preds = model.predict(x_val_multi, batch_size = BATCH_SIZE)\n",
    "# K.mean([multipred_acc(variable(test_preds[i]), variable(y_val_OH[i])) for i in range(len(y_val_OH))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uwOhXo3Oems"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1998010 samples, validate on 499520 samples\n",
      "Epoch 1/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 742.4946 - multipred_acc: 0.0401\n",
      "Epoch 00001: val_multipred_acc improved from -inf to 0.00751, saving model to ./trained_models_acc/mmi_classification/classifier_v2.h5\n",
      "1998010/1998010 [==============================] - 395s 197us/sample - loss: 742.4925 - multipred_acc: 0.0400 - val_loss: 769.1596 - val_multipred_acc: 0.0075\n",
      "Epoch 2/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 689.8465 - multipred_acc: 0.0396\n",
      "Epoch 00002: val_multipred_acc improved from 0.00751 to 0.02218, saving model to ./trained_models_acc/mmi_classification/classifier_v2.h5\n",
      "1998010/1998010 [==============================] - 376s 188us/sample - loss: 689.8279 - multipred_acc: 0.0396 - val_loss: 690.5864 - val_multipred_acc: 0.0222\n",
      "Epoch 3/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 666.2493 - multipred_acc: 0.0277\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00003: val_multipred_acc did not improve from 0.02218\n",
      "1998010/1998010 [==============================] - 352s 176us/sample - loss: 666.2489 - multipred_acc: 0.0277 - val_loss: 647.0374 - val_multipred_acc: 0.0091\n",
      "Epoch 4/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 642.7615 - multipred_acc: 0.0147\n",
      "Epoch 00004: val_multipred_acc improved from 0.02218 to 0.03356, saving model to ./trained_models_acc/mmi_classification/classifier_v2.h5\n",
      "1998010/1998010 [==============================] - 351s 176us/sample - loss: 642.7731 - multipred_acc: 0.0147 - val_loss: 645.0723 - val_multipred_acc: 0.0336\n",
      "Epoch 5/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 641.3811 - multipred_acc: 0.0396\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00005: val_multipred_acc did not improve from 0.03356\n",
      "1998010/1998010 [==============================] - 352s 176us/sample - loss: 641.4046 - multipred_acc: 0.0396 - val_loss: 638.7276 - val_multipred_acc: 0.0250\n",
      "Epoch 6/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 637.3968 - multipred_acc: 0.0600\n",
      "Epoch 00006: val_multipred_acc improved from 0.03356 to 0.06931, saving model to ./trained_models_acc/mmi_classification/classifier_v2.h5\n",
      "1998010/1998010 [==============================] - 352s 176us/sample - loss: 637.3934 - multipred_acc: 0.0600 - val_loss: 639.7478 - val_multipred_acc: 0.0693\n",
      "Epoch 7/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 636.3084 - multipred_acc: 0.0533\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00007: val_multipred_acc did not improve from 0.06931\n",
      "1998010/1998010 [==============================] - 352s 176us/sample - loss: 636.3012 - multipred_acc: 0.0534 - val_loss: 633.9941 - val_multipred_acc: 0.0240\n",
      "Epoch 8/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 635.4380 - multipred_acc: 0.0248\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00008: val_multipred_acc did not improve from 0.06931\n",
      "1998010/1998010 [==============================] - 348s 174us/sample - loss: 635.4376 - multipred_acc: 0.0248 - val_loss: 633.8978 - val_multipred_acc: 0.0088\n",
      "Epoch 9/50\n",
      "1996800/1998010 [============================>.] - ETA: 0s - loss: 633.0970 - multipred_acc: 0.0186Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00009: val_multipred_acc did not improve from 0.06931\n",
      "1998010/1998010 [==============================] - 347s 174us/sample - loss: 633.0912 - multipred_acc: 0.0186 - val_loss: 632.1774 - val_multipred_acc: 0.0157\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model_history = model.fit(x_train_multi, \n",
    "                          y_train_OH, \n",
    "                          epochs=EPOCHS,\n",
    "                          validation_data=(x_val_multi, y_val_OH),\n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          callbacks=callback_list,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKfQoBjQ5l7U"
   },
   "outputs": [],
   "source": [
    "plot_train_history(model_history, 'Classification Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(cur_file + '.h5')\n",
    "\n",
    "preds = model.predict(x_val_multi)\n",
    "print(preds.shape, preds.max(), preds.min())\n",
    "\n",
    "np.save(os.path.join(save_path, 'val_preds.npy'), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without normalized log(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/histStat_norm_class\"\n",
    "\n",
    "x_train_multi = np.load(os.path.join(data_path, \"X_train_unnorm.npy\"))\n",
    "x_val_multi = np.load(os.path.join(data_path, \"X_val_unnorm.npy\"))\n",
    "\n",
    "y_train_labels = np.load(os.path.join(data_path, \"y_train_labels.npy\"))\n",
    "y_val_labels = np.load(os.path.join(data_path, \"y_val_labels.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unnorm_Quake_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 15, 15)]          0         \n",
      "_________________________________________________________________\n",
      "LSTM1 (LSTM)                 (None, 15, 128)           73728     \n",
      "_________________________________________________________________\n",
      "LSTM2 (LSTM)                 (None, 15, 64)            49408     \n",
      "_________________________________________________________________\n",
      "LSTM3 (LSTM)                 (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "Dense_tseries (Dense)        (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "Dense_out (Dense)            (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 158,438\n",
      "Trainable params: 158,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ## Original Definition\n",
    "from tensorflow.keras.layers import LSTM, Dense, concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "## Unnorm model\n",
    "#Temporal sub-network for log(acc) \n",
    "tseries_in = Input(shape=x_train_multi.shape[1:])\n",
    "tseries = LSTM(128, return_sequences=True, input_shape=x_train_multi.shape[1:], name='LSTM1')(tseries_in)\n",
    "tseries = LSTM(64, return_sequences=True, activation='relu', name='LSTM2')(tseries)\n",
    "tseries = LSTM(64, activation='relu', name='LSTM3')(tseries)\n",
    "tseries = Dense(32, activation='relu', name='Dense_tseries')(tseries)\n",
    "tseries = Dense(num_classes, activation='softmax', name='Dense_out')(tseries)\n",
    "\n",
    "unnorm_model = Model(tseries_in, tseries, name=\"Unnorm_Quake_classifier\")\n",
    "\n",
    "unnorm_model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "unnorm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,CSVLogger\n",
    "from keras import optimizers\n",
    "\n",
    "lr = .01\n",
    "BATCH_SIZE=2048\n",
    "save_path = \"./trained_models_acc/classification\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# cur_file = \"LSTM3_10steps_dropzero\"\n",
    "cur_file = 'classifier_v1_unnorm'\n",
    "cur_file = os.path.join(save_path, cur_file)\n",
    "\n",
    "opt = optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "model_check = ModelCheckpoint(cur_file + '.h5', monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                              save_weights_only=True, mode='min')\n",
    "csv_log = CSVLogger(cur_file+ '_history.csv')\n",
    "callback_list = [early_stop, reduce_lr, model_check, csv_log]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 999005 samples, validate on 249760 samples\n",
      "Epoch 1/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.8759 - accuracy: 0.5883\n",
      "Epoch 00001: val_loss improved from inf to 0.70007, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 122s 122us/sample - loss: 0.8756 - accuracy: 0.5884 - val_loss: 0.7001 - val_accuracy: 0.6878\n",
      "Epoch 2/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.7107 - accuracy: 0.6679\n",
      "Epoch 00002: val_loss improved from 0.70007 to 0.63307, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.7106 - accuracy: 0.6680 - val_loss: 0.6331 - val_accuracy: 0.7213\n",
      "Epoch 3/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.6884\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.63307\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.6809 - accuracy: 0.6884 - val_loss: 0.7431 - val_accuracy: 0.6444\n",
      "Epoch 4/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.6032 - accuracy: 0.7412\n",
      "Epoch 00004: val_loss improved from 0.63307 to 0.57167, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.6032 - accuracy: 0.7412 - val_loss: 0.5717 - val_accuracy: 0.7581\n",
      "Epoch 5/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5875 - accuracy: 0.7490\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.57167\n",
      "999005/999005 [==============================] - 116s 117us/sample - loss: 0.5875 - accuracy: 0.7490 - val_loss: 0.5864 - val_accuracy: 0.7438\n",
      "Epoch 6/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5559 - accuracy: 0.7688\n",
      "Epoch 00006: val_loss improved from 0.57167 to 0.56121, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.5560 - accuracy: 0.7688 - val_loss: 0.5612 - val_accuracy: 0.7561\n",
      "Epoch 7/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5426 - accuracy: 0.7742\n",
      "Epoch 00007: val_loss improved from 0.56121 to 0.54693, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.5427 - accuracy: 0.7742 - val_loss: 0.5469 - val_accuracy: 0.7650\n",
      "Epoch 8/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5320 - accuracy: 0.7792\n",
      "Epoch 00008: val_loss improved from 0.54693 to 0.52056, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.5320 - accuracy: 0.7792 - val_loss: 0.5206 - val_accuracy: 0.7842\n",
      "Epoch 9/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7832\n",
      "Epoch 00009: val_loss improved from 0.52056 to 0.51857, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.5237 - accuracy: 0.7832 - val_loss: 0.5186 - val_accuracy: 0.7835\n",
      "Epoch 10/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5171 - accuracy: 0.7865\n",
      "Epoch 00010: val_loss improved from 0.51857 to 0.51289, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.5170 - accuracy: 0.7865 - val_loss: 0.5129 - val_accuracy: 0.7867\n",
      "Epoch 11/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5111 - accuracy: 0.7892\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51289\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.5111 - accuracy: 0.7892 - val_loss: 0.5130 - val_accuracy: 0.7873\n",
      "Epoch 12/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.7970\n",
      "Epoch 00012: val_loss improved from 0.51289 to 0.49966, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4965 - accuracy: 0.7970 - val_loss: 0.4997 - val_accuracy: 0.7954\n",
      "Epoch 13/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4943 - accuracy: 0.7978\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49966\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4943 - accuracy: 0.7978 - val_loss: 0.5024 - val_accuracy: 0.7933\n",
      "Epoch 14/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4880 - accuracy: 0.8008\n",
      "Epoch 00014: val_loss improved from 0.49966 to 0.49546, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4881 - accuracy: 0.8008 - val_loss: 0.4955 - val_accuracy: 0.7968\n",
      "Epoch 15/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4865 - accuracy: 0.8015\n",
      "Epoch 00015: val_loss improved from 0.49546 to 0.49305, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4866 - accuracy: 0.8015 - val_loss: 0.4931 - val_accuracy: 0.7975\n",
      "Epoch 16/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4853 - accuracy: 0.8017\n",
      "Epoch 00016: val_loss improved from 0.49305 to 0.49254, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4853 - accuracy: 0.8017 - val_loss: 0.4925 - val_accuracy: 0.7976\n",
      "Epoch 17/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4839 - accuracy: 0.8023\n",
      "Epoch 00017: val_loss improved from 0.49254 to 0.49115, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4839 - accuracy: 0.8023 - val_loss: 0.4912 - val_accuracy: 0.7983\n",
      "Epoch 18/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4828 - accuracy: 0.8028\n",
      "Epoch 00018: val_loss improved from 0.49115 to 0.49042, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4827 - accuracy: 0.8028 - val_loss: 0.4904 - val_accuracy: 0.7989\n",
      "Epoch 19/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4815 - accuracy: 0.8035\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49042\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4815 - accuracy: 0.8035 - val_loss: 0.4923 - val_accuracy: 0.7985\n",
      "Epoch 20/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.8046\n",
      "Epoch 00020: val_loss improved from 0.49042 to 0.48974, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4792 - accuracy: 0.8046 - val_loss: 0.4897 - val_accuracy: 0.7995\n",
      "Epoch 21/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4786 - accuracy: 0.8047\n",
      "Epoch 00021: val_loss improved from 0.48974 to 0.48803, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 116s 116us/sample - loss: 0.4786 - accuracy: 0.8047 - val_loss: 0.4880 - val_accuracy: 0.7997\n",
      "Epoch 22/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4781 - accuracy: 0.8049\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48803\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4781 - accuracy: 0.8049 - val_loss: 0.4881 - val_accuracy: 0.7995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4770 - accuracy: 0.8054\n",
      "Epoch 00023: val_loss improved from 0.48803 to 0.48676, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4770 - accuracy: 0.8054 - val_loss: 0.4868 - val_accuracy: 0.8006\n",
      "Epoch 24/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4767 - accuracy: 0.8055\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48676\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4767 - accuracy: 0.8055 - val_loss: 0.4873 - val_accuracy: 0.8001\n",
      "Epoch 25/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4762 - accuracy: 0.8058\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48676 to 0.48675, saving model to ./trained_models_acc/classification/classifier_v1_unnorm.h5\n",
      "999005/999005 [==============================] - 117s 117us/sample - loss: 0.4762 - accuracy: 0.8058 - val_loss: 0.4867 - val_accuracy: 0.8005\n",
      "Epoch 26/50\n",
      "509952/999005 [==============>...............] - ETA: 53s - loss: 0.4751 - accuracy: 0.8062"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "unnorm_model_history = unnorm_model.fit(x_train_multi, \n",
    "                                          y_train_labels, \n",
    "                                          epochs=EPOCHS,\n",
    "                                          validation_data=(x_val_multi, y_val_labels),\n",
    "                                          class_weight = class_weights,\n",
    "                                          batch_size = BATCH_SIZE, \n",
    "                                          callbacks=callback_list,\n",
    "                                           shuffle=True\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249760, 6) 1.0 7.657487e-35\n"
     ]
    }
   ],
   "source": [
    "unnorm_model.load_weights(cur_file + '.h5')\n",
    "\n",
    "preds = unnorm_model.predict(x_val_multi)\n",
    "print(preds.shape, preds.max(), preds.min())\n",
    "\n",
    "np.save(os.path.join(save_path, 'val_preds_unnorm.npy'), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model not trained on tseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/histStat_norm_class\"\n",
    "\n",
    "y_train_labels = np.load(os.path.join(data_path, \"y_train_labels.npy\"))\n",
    "y_val_labels = np.load(os.path.join(data_path, \"y_val_labels.npy\"))\n",
    "\n",
    "train_mean = np.squeeze(np.load(os.path.join(data_path, \"mean_train_histnorm.npy\")))\n",
    "train_std =  np.squeeze(np.load(os.path.join(data_path, \"std_train_histnorm.npy\")))\n",
    "\n",
    "val_mean = np.squeeze(np.load(os.path.join(data_path, \"mean_val_histnorm.npy\")))\n",
    "val_std =  np.squeeze(np.load(os.path.join(data_path, \"std_val_histnorm.npy\")))\n",
    "\n",
    "\n",
    "mean_scaler = preprocessing.StandardScaler().fit(train_mean)\n",
    "std_scaler = preprocessing.StandardScaler().fit(train_std)\n",
    "\n",
    "# x_train_multi = X_scaler.transform(x_train_multi)\n",
    "mean_scaled_train = mean_scaler.transform(train_mean)\n",
    "std_scaled_train = std_scaler.transform(train_std)\n",
    "\n",
    "# x_val_multi = X_scaler.transform(x_val_multi)\n",
    "mean_scaled_val = mean_scaler.transform(val_mean)\n",
    "std_scaled_val = std_scaler.transform(val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Nontemporal_Quake_classifier\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dense_mean (Dense)              (None, 16)           256         input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dense_std (Dense)               (None, 16)           256         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32)           0           Dense_mean[0][0]                 \n",
      "                                                                 Dense_std[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense_size (Dense)              (None, 64)           2112        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Dense_size_2 (Dense)            (None, 32)           2080        Dense_size[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "merged_out (Dense)              (None, 6)            198         Dense_size_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,902\n",
      "Trainable params: 4,902\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ## Original Definition\n",
    "from tensorflow.keras.layers import LSTM, Dense, concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "size1_in = Input(train_mean.shape[1:])\n",
    "size1 = Dense(16, activation='relu', name=\"Dense_mean\")(size1_in)\n",
    "# size_model1 = Model(size1_in, size1)\n",
    "\n",
    "size2_in = Input(train_mean.shape[1:])\n",
    "size2 = Dense(16, activation='relu', name=\"Dense_std\")(size2_in)\n",
    "# size_model2 = Model(size2_in, size2)\n",
    "\n",
    "size_branches = concatenate([size1, size2])\n",
    "size_branches = Dense(64, activation='relu', name=\"Dense_size\")(size_branches)\n",
    "size_branches = Dense(32, activation='relu', name=\"Dense_size_2\")(size_branches)\n",
    "\n",
    "out = Dense(num_classes, activation='softmax', name='merged_out')(size_branches)\n",
    "\n",
    "model_nontemporal = Model(inputs = [size1_in, size2_in], outputs = [out], name=\"Nontemporal_Quake_classifier\")\n",
    "\n",
    "\n",
    "model_nontemporal.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "model_nontemporal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,CSVLogger\n",
    "from keras import optimizers\n",
    "\n",
    "lr = .01\n",
    "BATCH_SIZE=2048\n",
    "save_path = \"./trained_models_acc/classification\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "# cur_file = \"LSTM3_10steps_dropzero\"\n",
    "cur_file = 'classifier_v1_nontemporal'\n",
    "cur_file = os.path.join(save_path, cur_file)\n",
    "\n",
    "opt = optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "model_check = ModelCheckpoint(cur_file + '.h5', monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                              save_weights_only=True, mode='min')\n",
    "csv_log = CSVLogger(cur_file+ '_history.csv')\n",
    "callback_list = [early_stop, reduce_lr, model_check, csv_log]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 999005 samples, validate on 249760 samples\n",
      "Epoch 1/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.6901 - accuracy: 0.7268\n",
      "Epoch 00001: val_loss improved from inf to 0.57042, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 14s 14us/sample - loss: 0.6895 - accuracy: 0.7270 - val_loss: 0.5704 - val_accuracy: 0.7650\n",
      "Epoch 2/50\n",
      "991232/999005 [============================>.] - ETA: 0s - loss: 0.5585 - accuracy: 0.7714\n",
      "Epoch 00002: val_loss improved from 0.57042 to 0.54332, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5583 - accuracy: 0.7715 - val_loss: 0.5433 - val_accuracy: 0.7762\n",
      "Epoch 3/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7783\n",
      "Epoch 00003: val_loss improved from 0.54332 to 0.53377, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 14s 14us/sample - loss: 0.5392 - accuracy: 0.7783 - val_loss: 0.5338 - val_accuracy: 0.7789\n",
      "Epoch 4/50\n",
      "991232/999005 [============================>.] - ETA: 0s - loss: 0.5297 - accuracy: 0.7819\n",
      "Epoch 00004: val_loss improved from 0.53377 to 0.52542, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5298 - accuracy: 0.7819 - val_loss: 0.5254 - val_accuracy: 0.7821\n",
      "Epoch 5/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5233 - accuracy: 0.7841\n",
      "Epoch 00005: val_loss improved from 0.52542 to 0.52199, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5233 - accuracy: 0.7841 - val_loss: 0.5220 - val_accuracy: 0.7837\n",
      "Epoch 6/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5185 - accuracy: 0.7862\n",
      "Epoch 00006: val_loss improved from 0.52199 to 0.52001, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5184 - accuracy: 0.7862 - val_loss: 0.5200 - val_accuracy: 0.7838\n",
      "Epoch 7/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5145 - accuracy: 0.7877\n",
      "Epoch 00007: val_loss improved from 0.52001 to 0.51541, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 12s 12us/sample - loss: 0.5145 - accuracy: 0.7877 - val_loss: 0.5154 - val_accuracy: 0.7866\n",
      "Epoch 8/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5110 - accuracy: 0.7891\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51541 to 0.51532, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5110 - accuracy: 0.7891 - val_loss: 0.5153 - val_accuracy: 0.7860\n",
      "Epoch 9/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5073 - accuracy: 0.7906\n",
      "Epoch 00009: val_loss improved from 0.51532 to 0.51116, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 17s 17us/sample - loss: 0.5073 - accuracy: 0.7906 - val_loss: 0.5112 - val_accuracy: 0.7888\n",
      "Epoch 10/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.5060 - accuracy: 0.7914\n",
      "Epoch 00010: val_loss improved from 0.51116 to 0.50984, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5059 - accuracy: 0.7914 - val_loss: 0.5098 - val_accuracy: 0.7893\n",
      "Epoch 11/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5047 - accuracy: 0.7918\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50984\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5047 - accuracy: 0.7918 - val_loss: 0.5106 - val_accuracy: 0.7894\n",
      "Epoch 12/50\n",
      "995328/999005 [============================>.] - ETA: 0s - loss: 0.5031 - accuracy: 0.7923\n",
      "Epoch 00012: val_loss improved from 0.50984 to 0.50852, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 11s 11us/sample - loss: 0.5031 - accuracy: 0.7923 - val_loss: 0.5085 - val_accuracy: 0.7901\n",
      "Epoch 13/50\n",
      "995328/999005 [============================>.] - ETA: 0s - loss: 0.5025 - accuracy: 0.7927\n",
      "Epoch 00013: val_loss improved from 0.50852 to 0.50838, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5025 - accuracy: 0.7928 - val_loss: 0.5084 - val_accuracy: 0.7901\n",
      "Epoch 14/50\n",
      "991232/999005 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.7931\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.50838\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5019 - accuracy: 0.7931 - val_loss: 0.5087 - val_accuracy: 0.7907\n",
      "Epoch 15/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5011 - accuracy: 0.7933\n",
      "Epoch 00015: val_loss improved from 0.50838 to 0.50737, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5011 - accuracy: 0.7933 - val_loss: 0.5074 - val_accuracy: 0.7906\n",
      "Epoch 16/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.7936\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.50737\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5008 - accuracy: 0.7936 - val_loss: 0.5076 - val_accuracy: 0.7909\n",
      "Epoch 17/50\n",
      "995328/999005 [============================>.] - ETA: 0s - loss: 0.5004 - accuracy: 0.7936\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.50737\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5004 - accuracy: 0.7936 - val_loss: 0.5074 - val_accuracy: 0.7910\n",
      "Epoch 18/50\n",
      "995328/999005 [============================>.] - ETA: 0s - loss: 0.5002 - accuracy: 0.7938\n",
      "Epoch 00018: val_loss improved from 0.50737 to 0.50726, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5001 - accuracy: 0.7938 - val_loss: 0.5073 - val_accuracy: 0.7910\n",
      "Epoch 19/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.5000 - accuracy: 0.7938\n",
      "Epoch 00019: val_loss improved from 0.50726 to 0.50713, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5001 - accuracy: 0.7938 - val_loss: 0.5071 - val_accuracy: 0.7913\n",
      "Epoch 20/50\n",
      "995328/999005 [============================>.] - ETA: 0s - loss: 0.5000 - accuracy: 0.7938\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.50713\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.5000 - accuracy: 0.7938 - val_loss: 0.5073 - val_accuracy: 0.7912\n",
      "Epoch 21/50\n",
      "991232/999005 [============================>.] - ETA: 0s - loss: 0.4998 - accuracy: 0.7939\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.50713 to 0.50710, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.4999 - accuracy: 0.7939 - val_loss: 0.5071 - val_accuracy: 0.7913\n",
      "Epoch 22/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.4998 - accuracy: 0.7939\n",
      "Epoch 00022: val_loss improved from 0.50710 to 0.50698, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.4998 - accuracy: 0.7939 - val_loss: 0.5070 - val_accuracy: 0.7912\n",
      "Epoch 23/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.4998 - accuracy: 0.7939\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.50698 to 0.50693, saving model to ./trained_models_acc/classification/classifier_v1_nontemporal.h5\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.4998 - accuracy: 0.7939 - val_loss: 0.5069 - val_accuracy: 0.7913\n",
      "Epoch 24/50\n",
      "993280/999005 [============================>.] - ETA: 0s - loss: 0.4998 - accuracy: 0.7939\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.50693\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.4997 - accuracy: 0.7939 - val_loss: 0.5070 - val_accuracy: 0.7913\n",
      "Epoch 25/50\n",
      "995328/999005 [============================>.] - ETA: 0s - loss: 0.4997 - accuracy: 0.7939\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.50693\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.4997 - accuracy: 0.7939 - val_loss: 0.5070 - val_accuracy: 0.7912\n",
      "Epoch 26/50\n",
      "997376/999005 [============================>.] - ETA: 0s - loss: 0.4997 - accuracy: 0.7939Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.50693\n",
      "999005/999005 [==============================] - 10s 10us/sample - loss: 0.4997 - accuracy: 0.7939 - val_loss: 0.5070 - val_accuracy: 0.7913\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model_nontemporal_history = model_nontemporal.fit([mean_scaled_train, std_scaled_train], \n",
    "                                          y_train_labels, \n",
    "                                          epochs=EPOCHS,\n",
    "                                          validation_data=([mean_scaled_val, std_scaled_val], y_val_labels),\n",
    "                                          class_weight = class_weights,\n",
    "                                          batch_size = BATCH_SIZE, \n",
    "                                          callbacks=callback_list,\n",
    "                                           shuffle=True\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249760, 6) 0.998317 2.1381644e-10\n"
     ]
    }
   ],
   "source": [
    "model_nontemporal.load_weights(cur_file + '.h5')\n",
    "\n",
    "preds = model_nontemporal.predict([mean_scaled_val, std_scaled_val])\n",
    "print(preds.shape, preds.max(), preds.min())\n",
    "\n",
    "np.save(os.path.join(save_path, 'val_preds_nontemporal.npy'), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
